{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀분석 학습 단계\n",
    "1. 학습데이터 준비 - 입출력 데이터 파이썬 형태로 읽기, 분리\n",
    "2. 임의의 직선 정의 - 수식생성\n",
    "3. 손실함수 정의\n",
    "4. 학습률 설정\n",
    "5. 최적값 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 학습데이터 준비\n",
    "x_data = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "t_data = np.array([2,3,4,5,6]).reshape(5,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.14795014]] , W.shape =  (1, 1) , b =  [0.04692587] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 직선 정의 => 임의의 값으로 초기화\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 정의\n",
    "def loss_func(x,t):\n",
    "    y = np.dot(x,W) + b\n",
    "    return (np.sum((t-y)**2)) / (len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치미분 함수 정의\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return ( np.sum( (t - y)**2 ) ) / ( len(x) )\n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 x : numpy type\n",
    "def predict(x):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  13.766628902255547 Initial W =  [[0.14795014]] \n",
      " , b =  [0.04692587]\n",
      "step =  0 error value =  8.13527704148347 W =  [[0.39258556]] , b =  [0.10243222]\n",
      "step =  400 error value =  0.0060713735839439936 W =  [[1.0505988]] , b =  [0.81736699]\n",
      "step =  800 error value =  0.00038738952845131794 W =  [[1.01278117]] , b =  [0.95386721]\n",
      "step =  1200 error value =  2.4717742151559284e-05 W =  [[1.0032285]] , b =  [0.98834693]\n",
      "step =  1600 error value =  1.5771380798904015e-06 W =  [[1.00081551]] , b =  [0.99705646]\n",
      "step =  2000 error value =  1.006307334944973e-07 W =  [[1.000206]] , b =  [0.99925647]\n",
      "step =  2400 error value =  6.420835723150125e-09 W =  [[1.00005203]] , b =  [0.99981218]\n",
      "step =  2800 error value =  4.0968727894981086e-10 W =  [[1.00001314]] , b =  [0.99995256]\n",
      "step =  3200 error value =  2.61404704588713e-11 W =  [[1.00000332]] , b =  [0.99998802]\n",
      "step =  3600 error value =  1.6679165567103822e-12 W =  [[1.00000084]] , b =  [0.99999697]\n",
      "step =  4000 error value =  1.0642293708857469e-13 W =  [[1.00000021]] , b =  [0.99999924]\n",
      "step =  4400 error value =  6.790412587998421e-15 W =  [[1.00000005]] , b =  [0.99999981]\n",
      "step =  4800 error value =  4.3326847024830365e-16 W =  [[1.00000001]] , b =  [0.99999995]\n",
      "step =  5200 error value =  2.7645092040972913e-17 W =  [[1.]] , b =  [0.99999999]\n",
      "step =  5600 error value =  1.7639209001809107e-18 W =  [[1.]] , b =  [1.]\n",
      "step =  6000 error value =  1.125483783831094e-19 W =  [[1.]] , b =  [1.]\n",
      "step =  6400 error value =  7.181218840121272e-21 W =  [[1.]] , b =  [1.]\n",
      "step =  6800 error value =  4.582034240412339e-22 W =  [[1.]] , b =  [1.]\n",
      "step =  7200 error value =  2.9237785745793893e-23 W =  [[1.]] , b =  [1.]\n",
      "step =  7600 error value =  1.8663804912600027e-24 W =  [[1.]] , b =  [1.]\n",
      "step =  8000 error value =  1.1959293039636108e-25 W =  [[1.]] , b =  [1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 1e-2  # 발산하는 경우, 1e-3 ~ 1e-6 등으로 바꾸어서 실행\n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "\n",
    "print(\"Initial error value = \", error_val(x_data, t_data), \"Initial W = \", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "for step in  range(8001):  \n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \",b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.ndim =  2 , x_data.shape =  (25, 3)\n",
      "t_data.ndim =  2 , t_data.shape =  (25, 1)\n"
     ]
    }
   ],
   "source": [
    "# 학습데이터 준비\n",
    "loaded_data = np.loadtxt(r'C:\\Users\\01048\\Desktop\\machine_learning\\DeepLearning\\BasicPython\\data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "x_data = loaded_data[:,0:-1]\n",
    "t_data = loaded_data[:,[-1]]\n",
    "print(\"x_data.ndim = \", x_data.ndim, \", x_data.shape = \", x_data.shape)\n",
    "print(\"t_data.ndim = \", t_data.ndim, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.80838242]\n",
      " [0.99082187]\n",
      " [0.39571062]] , W.shape =  (3, 1) , b =  [0.0204637] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 직선 정의\n",
    "W = np.random.rand(3,1)\n",
    "b = np.random.rand(1)\n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return ( np.sum( (t - y)**2 ) ) / ( len(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return ( np.sum( (t - y)**2 ) ) / ( len(x) )\n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 x : numpy type\n",
    "def predict(x):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value=  194.98805589955143 \n",
      "initial W =  [[0.80838242]\n",
      " [0.99082187]\n",
      " [0.39571062]] \n",
      "b =  [0.0204637]\n",
      "step =  0 error value =  87.92752876197092 W =  [[0.78745062]\n",
      " [0.96966379]\n",
      " [0.37459983]] , b =  [0.02030746]\n",
      "step =  400 error value =  19.72543300033271 W =  [[0.71473381]\n",
      " [0.85782998]\n",
      " [0.45816472]] , b =  [0.02025588]\n",
      "step =  800 error value =  15.885371721448164 W =  [[0.67856657]\n",
      " [0.79265216]\n",
      " [0.55700572]] , b =  [0.02032033]\n",
      "step =  1200 error value =  13.151729135284027 W =  [[0.64617249]\n",
      " [0.73921754]\n",
      " [0.64070548]] , b =  [0.0202769]\n",
      "step =  1600 error value =  11.20111744523736 W =  [[0.6171384 ]\n",
      " [0.69550595]\n",
      " [0.71163949]] , b =  [0.02014203]\n",
      "step =  2000 error value =  9.805533890759605 W =  [[0.5910993 ]\n",
      " [0.65983651]\n",
      " [0.77180486]] , b =  [0.01992959]\n",
      "step =  2400 error value =  8.804066058069145 W =  [[0.56773229]\n",
      " [0.63081141]\n",
      " [0.82288122]] , b =  [0.0196513]\n",
      "step =  2800 error value =  8.083019911232899 W =  [[0.54675139]\n",
      " [0.60726891]\n",
      " [0.86628165]] , b =  [0.01931704]\n",
      "step =  3200 error value =  7.561957891144655 W =  [[0.527903  ]\n",
      " [0.5882441 ]\n",
      " [0.90319544]] , b =  [0.01893517]\n",
      "step =  3600 error value =  7.183884982310441 W =  [[0.51096199]\n",
      " [0.57293606]\n",
      " [0.93462396]] , b =  [0.01851277]\n",
      "step =  4000 error value =  6.908346824363665 W =  [[0.49572825]\n",
      " [0.56068057]\n",
      " [0.96141068]] , b =  [0.01805583]\n",
      "step =  4400 error value =  6.7065728555663 W =  [[0.48202378]\n",
      " [0.55092718]\n",
      " [0.98426645]] , b =  [0.01756942]\n",
      "step =  4800 error value =  6.558056150506766 W =  [[0.46969003]\n",
      " [0.54322019]\n",
      " [1.00379061]] , b =  [0.01705784]\n",
      "step =  5200 error value =  6.448143015265946 W =  [[0.45858569]\n",
      " [0.53718272]\n",
      " [1.02048876]] , b =  [0.01652474]\n",
      "step =  5600 error value =  6.366332632282628 W =  [[0.44858464]\n",
      " [0.53250342]\n",
      " [1.03478767]] , b =  [0.01597321]\n",
      "step =  6000 error value =  6.305076296018893 W =  [[0.43957428]\n",
      " [0.52892541]\n",
      " [1.04704777]] , b =  [0.01540589]\n",
      "step =  6400 error value =  6.258928398942239 W =  [[0.43145395]\n",
      " [0.52623705]\n",
      " [1.05757367]] , b =  [0.01482503]\n",
      "step =  6800 error value =  6.223945271105585 W =  [[0.42413362]\n",
      " [0.52426426]\n",
      " [1.06662297]] , b =  [0.01423253]\n",
      "step =  7200 error value =  6.197258823021624 W =  [[0.41753269]\n",
      " [0.52286406]\n",
      " [1.07441368]] , b =  [0.01363001]\n",
      "step =  7600 error value =  6.176773600710373 W =  [[0.41157898]\n",
      " [0.52191932]\n",
      " [1.08113043]] , b =  [0.01301888]\n",
      "step =  8000 error value =  6.160951075381969 W =  [[0.40620776]\n",
      " [0.52133422]\n",
      " [1.08692971]] , b =  [0.01240032]\n",
      "step =  8400 error value =  6.148655680689119 W =  [[0.401361  ]\n",
      " [0.52103063]\n",
      " [1.09194427]] , b =  [0.01177535]\n",
      "step =  8800 error value =  6.139044626148381 W =  [[0.3969866 ]\n",
      " [0.52094501]\n",
      " [1.09628684]] , b =  [0.01114483]\n",
      "step =  9200 error value =  6.131488801922801 W =  [[0.39303778]\n",
      " [0.52102585]\n",
      " [1.10005319]] , b =  [0.01050951]\n",
      "step =  9600 error value =  6.125515811156816 W =  [[0.38947251]\n",
      " [0.52123154]\n",
      " [1.10332483]] , b =  [0.00987004]\n",
      "step =  10000 error value =  6.120768787032956 W =  [[0.38625302]\n",
      " [0.52152864]\n",
      " [1.10617114]] , b =  [0.00922697]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "f = lambda x: loss_func(x_data,t_data)\n",
    "print('initial error value= ', error_val(x_data,t_data), '\\ninitial W = ',W,'\\nb = ',b)\n",
    "\n",
    "for step in range(10001):\n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step%400) == 0:\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \",b )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
