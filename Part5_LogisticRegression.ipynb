{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape =  (10, 1) , t_data.shape =  (10, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]).reshape(10,1)   \n",
    "t_data = np.array([0, 0, 0, 0,  0,  0,  1,  1,  1,  1]).reshape(10,1)\n",
    "\n",
    "print(\"x_data.shape = \", x_data.shape, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.49786465]] , W.shape =  (1, 1) , b =  [0.46440725] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "b = np.random.rand(1)  \n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3. 손실함수 정의\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def loss_func(x, t):\n",
    "    \n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steop4. 수치미분함수\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_val(x, t):\n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 x : numpy type\n",
    "def predict(x):\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    if y >= 0.5:\n",
    "        result = 1  # True\n",
    "    else:\n",
    "        result = 0  # False\n",
    "    \n",
    "    return y, result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  23.994175297332696 Initial W =  [[0.56818582]\n",
      " [0.3838152 ]] \n",
      " , b =  [0.65319094]\n",
      "step =  0 error value =  13.332709717268383 W =  [[0.36911478]\n",
      " [0.12546809]] , b =  [0.6152791]\n",
      "step =  400 error value =  2.295283099539854 W =  [[ 0.41756451]\n",
      " [-0.08862594]] , b =  [-2.5808781]\n",
      "step =  800 error value =  1.5997619323508485 W =  [[ 0.53427752]\n",
      " [-0.02728512]] , b =  [-4.2360795]\n",
      "step =  1200 error value =  1.2845138387349064 W =  [[0.62183374]\n",
      " [0.00842014]] , b =  [-5.35503425]\n",
      "step =  1600 error value =  1.1010509085277056 W =  [[0.69243055]\n",
      " [0.03377653]] , b =  [-6.20958609]\n",
      "step =  2000 error value =  0.9786758460403969 W =  [[0.75189191]\n",
      " [0.05392973]] , b =  [-6.90773643]\n",
      "step =  2400 error value =  0.889760614634417 W =  [[0.8034224 ]\n",
      " [0.07115278]] , b =  [-7.50288018]\n",
      "step =  2800 error value =  0.8212855280316265 W =  [[0.84897578]\n",
      " [0.08661792]] , b =  [-8.02515098]\n",
      "step =  3200 error value =  0.7662988989257249 W =  [[0.88983527]\n",
      " [0.10099155]] , b =  [-8.49314765]\n",
      "step =  3600 error value =  0.7207399842227501 W =  [[0.92689278]\n",
      " [0.11467731]] , b =  [-8.91912213]\n",
      "step =  4000 error value =  0.6820700180320608 W =  [[0.96079675]\n",
      " [0.12792926]] , b =  [-9.31155668]\n",
      "step =  4400 error value =  0.648615099429445 W =  [[0.99203654]\n",
      " [0.14091   ]] , b =  [-9.6765585]\n",
      "step =  4800 error value =  0.6192234180874062 W =  [[1.02099331]\n",
      " [0.15372277]] , b =  [-10.01866622]\n",
      "step =  5200 error value =  0.5930741896086195 W =  [[1.04797215]\n",
      " [0.16643066]] , b =  [-10.34134176]\n",
      "step =  5600 error value =  0.569565215029107 W =  [[1.07322292]\n",
      " [0.17906868]] , b =  [-10.64728386]\n",
      "step =  6000 error value =  0.5482436417535911 W =  [[1.09695427]\n",
      " [0.19165204]] , b =  [-10.93863539]\n",
      "step =  6400 error value =  0.5287616546712968 W =  [[1.1193431 ]\n",
      " [0.20418208]] , b =  [-11.21712493]\n",
      "step =  6800 error value =  0.5108471675892982 W =  [[1.14054113]\n",
      " [0.21665073]] , b =  [-11.48416617]\n",
      "step =  7200 error value =  0.49428387274032753 W =  [[1.16067954]\n",
      " [0.22904397]] , b =  [-11.7409294]\n",
      "step =  7600 error value =  0.4788973166395227 W =  [[1.17987235]\n",
      " [0.24134451]] , b =  [-11.988394]\n",
      "step =  8000 error value =  0.46454496740913676 W =  [[1.19821893]\n",
      " [0.25353383]] , b =  [-12.22738778]\n",
      "step =  8400 error value =  0.45110899285159123 W =  [[1.21580601]\n",
      " [0.26559371]] , b =  [-12.45861687]\n",
      "step =  8800 error value =  0.4384909210544171 W =  [[1.23270925]\n",
      " [0.27750725]] , b =  [-12.68268879]\n",
      "step =  9200 error value =  0.4266076345327713 W =  [[1.24899466]\n",
      " [0.28925954]] , b =  [-12.90013042]\n",
      "step =  9600 error value =  0.4153883256893407 W =  [[1.26471977]\n",
      " [0.30083799]] , b =  [-13.11140217]\n",
      "step =  10000 error value =  0.4047721559904985 W =  [[1.27993474]\n",
      " [0.31223249]] , b =  [-13.31690932]\n"
     ]
    }
   ],
   "source": [
    "# stpe5 학습률 조정\n",
    "learning_rate = 1e-2  # 발산하는 경우, 1e-3 ~ 1e-6 등으로 바꾸어서 실행\n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)  # f(x) = loss_func(x_data, t_data)\n",
    "\n",
    "print(\"Initial error value = \", error_val(x_data, t_data), \"Initial W = \", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "for step in  range(10001):  \n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \",b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1146732e-05]] 0\n"
     ]
    }
   ],
   "source": [
    "(real_val, logical_val) = predict(3)\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99130321]] 1\n"
     ]
    }
   ],
   "source": [
    "(real_val, logical_val) = predict(17)\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.ndim =  2 , x_data.shape =  (9, 2)\n",
      "t_data.ndim =  2 , t_data.shape =  (9, 1)\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([ [2, 4], [4, 11], [6, 6], [8, 5], [10, 7], [12, 16], [14, 8], [16, 3], [18, 7] ])\n",
    "t_data = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1]).reshape(9, 1)\n",
    "print(\"x_data.ndim = \", x_data.ndim, \", x_data.shape = \", x_data.shape)\n",
    "print(\"t_data.ndim = \", t_data.ndim, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.56818582]\n",
      " [0.3838152 ]] , W.shape =  (2, 1) , b =  [0.65319094] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(2, 1)  # 2X1 행렬\n",
    "b = np.random.rand(1)  \n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01522982]), 0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([3,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88516803]), 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([12,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND, OR, NAND, XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogicGate Class\n",
    "\n",
    "class LogicGate:\n",
    "    \n",
    "    def __init__(self, gate_name, xdata, tdata):  # xdata, tdata => numpy.array(...)\n",
    "        \n",
    "        self.name = gate_name\n",
    "        \n",
    "        # 입력 데이터, 정답 데이터 초기화, 외부에서 쉽게 변경할수 없도록 private(__variable)설정\n",
    "        self.__xdata = xdata.reshape(4,2)\n",
    "        self.__tdata = tdata.reshape(4,1)\n",
    "        \n",
    "        # 가중치 W, 바이어스 b 초기화\n",
    "        self.__W = np.random.rand(2,1)  # weight, 2 X 1 matrix\n",
    "        self.__b = np.random.rand(1)\n",
    "                        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.__learning_rate = 1e-2\n",
    "        \n",
    "    # 손실함수\n",
    "    def __loss_func(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z = np.dot(self.__xdata, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1 - y)+delta ) )      \n",
    "    \n",
    "    # 손실 값 계산\n",
    "    def error_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z = np.dot(self.__xdata, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1 - y)+delta ) )\n",
    "\n",
    "    # 수치미분을 이용하여 손실함수가 최소가 될때 까지 학습하는 함수\n",
    "    def train(self):\n",
    "        \n",
    "        f = lambda x : self.__loss_func()\n",
    "        \n",
    "        print(\"Initial error value = \", self.error_val())\n",
    "        \n",
    "        for step in  range(8001):\n",
    "            \n",
    "            self.__W -= self.__learning_rate * numerical_derivative(f, self.__W)\n",
    "    \n",
    "            self.__b -= self.__learning_rate * numerical_derivative(f, self.__b)\n",
    "    \n",
    "            if (step % 400 == 0):\n",
    "                print(\"step = \", step, \"error value = \", self.error_val())\n",
    "                \n",
    "                \n",
    "    # 미래 값 예측 함수\n",
    "    def predict(self, input_data):\n",
    "        \n",
    "        z = np.dot(input_data, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "    \n",
    "        if y > 0.5:\n",
    "            result = 1  # True\n",
    "        else:\n",
    "            result = 0  # False\n",
    "    \n",
    "        return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  3.2535870043831547\n",
      "step =  0 error value =  3.227911350726375\n",
      "step =  400 error value =  1.567589839768646\n",
      "step =  800 error value =  1.157978790058473\n",
      "step =  1200 error value =  0.9282439130424094\n",
      "step =  1600 error value =  0.7775136002817833\n",
      "step =  2000 error value =  0.6695613618424223\n",
      "step =  2400 error value =  0.5879047076718696\n",
      "step =  2800 error value =  0.5237840263091686\n",
      "step =  3200 error value =  0.47203118660412613\n",
      "step =  3600 error value =  0.42936449941913635\n",
      "step =  4000 error value =  0.39358453011000416\n",
      "step =  4400 error value =  0.3631550897572004\n",
      "step =  4800 error value =  0.3369676647968354\n",
      "step =  5200 error value =  0.3142008633203752\n",
      "step =  5600 error value =  0.29423247222810506\n",
      "step =  6000 error value =  0.27658225811200843\n",
      "step =  6400 error value =  0.26087354685577846\n",
      "step =  6800 error value =  0.24680670520610065\n",
      "step =  7200 error value =  0.23414040544582074\n",
      "step =  7600 error value =  0.2226781180625605\n",
      "step =  8000 error value =  0.21225819883632607\n"
     ]
    }
   ],
   "source": [
    "# AND\n",
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 0, 0, 1])\n",
    "\n",
    "AND_obj = LogicGate(\"AND_GATE\", xdata, tdata)\n",
    "\n",
    "AND_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND_GATE \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  0 \n",
      "\n",
      "[1 0]  =  0 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# AND Gate prediction\n",
    "print(AND_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = AND_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  1.890309352306683\n",
      "step =  0 error value =  1.8831271896343367\n",
      "step =  400 error value =  1.0912515266597484\n",
      "step =  800 error value =  0.7933749337695576\n",
      "step =  1200 error value =  0.6176571369682389\n",
      "step =  1600 error value =  0.5027650386053997\n",
      "step =  2000 error value =  0.4222564852636911\n",
      "step =  2400 error value =  0.3629790656824774\n",
      "step =  2800 error value =  0.3176735840970382\n",
      "step =  3200 error value =  0.28201793557503163\n",
      "step =  3600 error value =  0.25328599816916914\n",
      "step =  4000 error value =  0.22967804817189996\n",
      "step =  4400 error value =  0.2099609458791136\n",
      "step =  4800 error value =  0.19326317348241623\n",
      "step =  5200 error value =  0.17895238800305668\n",
      "step =  5600 error value =  0.16655929929310853\n",
      "step =  6000 error value =  0.15572871593532212\n",
      "step =  6400 error value =  0.14618712776547\n",
      "step =  6800 error value =  0.13772068285689398\n",
      "step =  7200 error value =  0.1301598846090807\n",
      "step =  7600 error value =  0.12336874279753775\n",
      "step =  8000 error value =  0.11723694273535097\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 1, 1, 1])\n",
    "\n",
    "OR_obj = LogicGate(\"OR_GATE\", xdata, tdata)\n",
    "\n",
    "OR_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR_GATE \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR Gate prediction\n",
    "print(OR_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = OR_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  2.694221708931494\n",
      "step =  0 error value =  2.6896125796833568\n",
      "step =  400 error value =  1.6140612049853327\n",
      "step =  800 error value =  1.180256806833635\n",
      "step =  1200 error value =  0.9417557054446914\n",
      "step =  1600 error value =  0.7867880151883676\n",
      "step =  2000 error value =  0.6763958781221359\n",
      "step =  2400 error value =  0.593176460530256\n",
      "step =  2800 error value =  0.5279831068562958\n",
      "step =  3200 error value =  0.4754575575537403\n",
      "step =  3600 error value =  0.43221395706893817\n",
      "step =  4000 error value =  0.3959911184958525\n",
      "step =  4400 error value =  0.3652140188631461\n",
      "step =  4800 error value =  0.33874858067536795\n",
      "step =  5200 error value =  0.31575595824370195\n",
      "step =  5600 error value =  0.2956016605231208\n",
      "step =  6000 error value =  0.2777965985939182\n",
      "step =  6400 error value =  0.26195757392711555\n",
      "step =  6800 error value =  0.24778005712043422\n",
      "step =  7200 error value =  0.2350189866131919\n",
      "step =  7600 error value =  0.22347494462028192\n",
      "step =  8000 error value =  0.2129840241753408\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([1, 1, 1, 0])\n",
    "\n",
    "NAND_obj = LogicGate(\"NAND_GATE\", xdata, tdata)\n",
    "\n",
    "NAND_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND_GATE \n",
      "\n",
      "[0 0]  =  1 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NAND Gate prediction\n",
    "print(NAND_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = NAND_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  3.6142637271770806\n",
      "step =  0 error value =  3.593412011023175\n",
      "step =  400 error value =  2.774393822120537\n",
      "step =  800 error value =  2.7729401365193382\n",
      "step =  1200 error value =  2.7726662798945454\n",
      "step =  1600 error value =  2.77260723180506\n",
      "step =  2000 error value =  2.7725930036498494\n",
      "step =  2400 error value =  2.7725893095913863\n",
      "step =  2800 error value =  2.7725883083136567\n",
      "step =  3200 error value =  2.7725880306988757\n",
      "step =  3600 error value =  2.772587952849568\n",
      "step =  4000 error value =  2.7725879308977834\n",
      "step =  4400 error value =  2.7725879246913396\n",
      "step =  4800 error value =  2.772587922934345\n",
      "step =  5200 error value =  2.7725879224366508\n",
      "step =  5600 error value =  2.77258792229563\n",
      "step =  6000 error value =  2.772587922255667\n",
      "step =  6400 error value =  2.7725879222443415\n",
      "step =  6800 error value =  2.772587922241131\n",
      "step =  7200 error value =  2.772587922240221\n",
      "step =  7600 error value =  2.7725879222399636\n",
      "step =  8000 error value =  2.7725879222398904\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 1, 1, 0])\n",
    "\n",
    "\n",
    "XOR_obj = LogicGate(\"XOR_GATE\", xdata, tdata)\n",
    "\n",
    "# XOR Gate 를 보면, 손실함수 값이 2.7 근처에서 더 이상 감소하지 않는것을 볼수 있음\n",
    "XOR_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR_GATE \n",
      "\n",
      "[0 0]  =  1 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# error value가 감소하지 않음\n",
    "# XOR Gate prediction => 예측이 되지 않음\n",
    "print(XOR_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = XOR_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구현이 안된다는것을 확인할 수 있음\n",
    "- NAND, OR, AND를 결합해서 구현해야한다\n",
    "- Multi layer로 해결해야함\n",
    "- 이전 게이트의 출력은 다음게이트의 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]  =  0\n",
      "\n",
      "[0 1]  =  1\n",
      "\n",
      "[1 0]  =  1\n",
      "\n",
      "[1 1]  =  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# XOR 을 NAND + OR => AND 조합으로 계산함\n",
    "input_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "s1 = []    # NAND 출력\n",
    "s2 = []    # OR 출력\n",
    "\n",
    "new_input_data = []  # AND 입력\n",
    "final_output = []    # AND 출력\n",
    "\n",
    "for index in range(len(input_data)):\n",
    "    \n",
    "    s1 = NAND_obj.predict(input_data[index])  # NAND 출력\n",
    "    s2 = OR_obj.predict(input_data[index])    # OR 출력\n",
    "    \n",
    "    new_input_data.append(s1[-1])    # AND 입력\n",
    "    new_input_data.append(s2[-1])    # AND 입력\n",
    "    \n",
    "    (sigmoid_val, logical_val) = AND_obj.predict(np.array(new_input_data))\n",
    "    \n",
    "    final_output.append(logical_val)    # AND 출력, 즉 XOR 출력    \n",
    "    new_input_data = []    # AND 입력 초기화\n",
    "\n",
    "\n",
    "for index in range(len(input_data)):    \n",
    "    print(input_data[index], \" = \", final_output[index], end='')\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
